{
  "$schema": "https://ui.shadcn.com/schema/registry-item.json",
  "name": "use-audio-level",
  "type": "registry:hook",
  "title": "useAudioLevel",
  "description": "A visualization hook for analyzing audio volume.",
  "categories": [
    "webrtc"
  ],
  "files": [
    {
      "path": "webrtc/use-audio-level.ts",
      "type": "registry:hook",
      "content": "import { useState, useEffect, useRef, useCallback } from \"react\";\n\n/**\n * Options for the useAudioLevel hook\n */\nexport interface UseAudioLevelOptions {\n    /** Threshold (0-1) above which isSpeaking is true (default: 0.01) */\n    speakingThreshold?: number;\n    /** How often to sample audio level in ms (default: 100) */\n    sampleInterval?: number;\n    /** Smoothing factor for level transitions (0-1, default: 0.8) */\n    smoothingFactor?: number;\n    /** Whether to start monitoring automatically (default: true) */\n    autoStart?: boolean;\n}\n\n/**\n * Return type for the useAudioLevel hook\n */\nexport interface UseAudioLevelReturn {\n    /** Current audio level (0-1) */\n    level: number;\n    /** Whether audio exceeds the speaking threshold */\n    isSpeaking: boolean;\n    /** Peak audio level since last reset */\n    peak: number;\n    /** Reset peak level */\n    resetPeak: () => void;\n    /** Start monitoring audio */\n    start: () => void;\n    /** Stop monitoring audio */\n    stop: () => void;\n    /** Whether currently monitoring */\n    isMonitoring: boolean;\n    /** Whether Web Audio API is supported */\n    isSupported: boolean;\n}\n\n/**\n * A React hook for detecting audio volume from a media stream.\n * Uses Web Audio API to analyze audio levels for speaking indicators.\n *\n * @param stream - The MediaStream to monitor (must have audio tracks)\n * @param options - Configuration options\n * @returns UseAudioLevelReturn object with level, speaking state, and controls\n *\n * @example\n * ```tsx\n * const { stream } = useUserMedia({ audio: true });\n * const { level, isSpeaking } = useAudioLevel(stream);\n *\n * return (\n *     <div>\n *         <div\n *             className=\"speaking-indicator\"\n *             style={{\n *                 opacity: isSpeaking ? 1 : 0.3,\n *                 transform: `scale(${1 + level * 0.5})`,\n *             }}\n *         />\n *         <p>Volume: {Math.round(level * 100)}%</p>\n *     </div>\n * );\n * ```\n */\nexport function useAudioLevel(\n    stream: MediaStream | null,\n    options: UseAudioLevelOptions = {},\n): UseAudioLevelReturn {\n    const {\n        speakingThreshold = 0.01,\n        sampleInterval = 100,\n        smoothingFactor = 0.8,\n        autoStart = true,\n    } = options;\n\n    const [level, setLevel] = useState(0);\n    const [peak, setPeak] = useState(0);\n    const [isMonitoring, setIsMonitoring] = useState(false);\n\n    // Refs for audio context and nodes\n    const audioContextRef = useRef<AudioContext | null>(null);\n    const analyserRef = useRef<AnalyserNode | null>(null);\n    const sourceRef = useRef<MediaStreamAudioSourceNode | null>(null);\n    const animationFrameRef = useRef<number | null>(null);\n    const intervalRef = useRef<ReturnType<typeof setInterval> | null>(null);\n    const smoothedLevelRef = useRef(0);\n\n    // Check if Web Audio API is supported\n    const isSupported =\n        typeof window !== \"undefined\" &&\n        (\"AudioContext\" in window || \"webkitAudioContext\" in window);\n\n    // Calculate RMS (Root Mean Square) level from audio data\n    const calculateLevel = useCallback((): number => {\n        if (!analyserRef.current) return 0;\n\n        const analyser = analyserRef.current;\n        const dataArray = new Uint8Array(analyser.frequencyBinCount);\n        analyser.getByteTimeDomainData(dataArray);\n\n        // Calculate RMS\n        let sum = 0;\n        for (let i = 0; i < dataArray.length; i++) {\n            const value = dataArray[i] ?? 128;\n            const normalized = (value - 128) / 128;\n            sum += normalized * normalized;\n        }\n        const rms = Math.sqrt(sum / dataArray.length);\n\n        return Math.min(1, rms * 2); // Scale up and clamp\n    }, []);\n\n    // Start monitoring\n    const start = useCallback(() => {\n        if (!stream || !isSupported || isMonitoring) return;\n\n        const audioTrack = stream.getAudioTracks()[0];\n        if (!audioTrack) return;\n\n        try {\n            // Create audio context\n            const AudioContextClass =\n                window.AudioContext ||\n                (\n                    window as unknown as {\n                        webkitAudioContext: typeof AudioContext;\n                    }\n                ).webkitAudioContext;\n            const audioContext = new AudioContextClass();\n            audioContextRef.current = audioContext;\n\n            // Resume context if suspended (browsers require user gesture)\n            if (audioContext.state === \"suspended\") {\n                audioContext.resume().catch(() => {\n                    console.warn(\"Failed to resume AudioContext\");\n                });\n            }\n\n            // Create analyser node\n            const analyser = audioContext.createAnalyser();\n            analyser.fftSize = 256;\n            analyser.smoothingTimeConstant = smoothingFactor;\n            analyserRef.current = analyser;\n\n            // Create source from stream\n            const source = audioContext.createMediaStreamSource(stream);\n            source.connect(analyser);\n            sourceRef.current = source;\n\n            // Start sampling at interval\n            intervalRef.current = setInterval(() => {\n                // Ensure context is running\n                if (audioContextRef.current?.state === \"suspended\") {\n                    audioContextRef.current.resume().catch(() => {});\n                }\n\n                const rawLevel = calculateLevel();\n\n                // Apply smoothing\n                smoothedLevelRef.current =\n                    smoothedLevelRef.current * smoothingFactor +\n                    rawLevel * (1 - smoothingFactor);\n\n                const currentLevel = smoothedLevelRef.current;\n                setLevel(currentLevel);\n\n                // Update peak\n                if (currentLevel > peak) {\n                    setPeak(currentLevel);\n                }\n            }, sampleInterval);\n\n            setIsMonitoring(true);\n        } catch (err) {\n            console.error(\"Failed to start audio monitoring:\", err);\n        }\n    }, [\n        stream,\n        isSupported,\n        isMonitoring,\n        sampleInterval,\n        smoothingFactor,\n        calculateLevel,\n        peak,\n    ]);\n\n    // Stop monitoring\n    const stop = useCallback(() => {\n        if (intervalRef.current) {\n            clearInterval(intervalRef.current);\n            intervalRef.current = null;\n        }\n\n        if (animationFrameRef.current) {\n            cancelAnimationFrame(animationFrameRef.current);\n            animationFrameRef.current = null;\n        }\n\n        if (sourceRef.current) {\n            sourceRef.current.disconnect();\n            sourceRef.current = null;\n        }\n\n        if (audioContextRef.current) {\n            audioContextRef.current.close().catch(() => {});\n            audioContextRef.current = null;\n        }\n\n        analyserRef.current = null;\n        smoothedLevelRef.current = 0;\n        setLevel(0);\n        setIsMonitoring(false);\n    }, []);\n\n    // Reset peak\n    const resetPeak = useCallback(() => {\n        setPeak(0);\n    }, []);\n\n    // Auto-start when stream changes, stop when stream is null\n    useEffect(() => {\n        // If stream is null or undefined, stop monitoring\n        if (!stream) {\n            if (isMonitoring) {\n                // Clean up manually instead of calling stop to avoid dependency issues\n                if (intervalRef.current) {\n                    clearInterval(intervalRef.current);\n                    intervalRef.current = null;\n                }\n                if (animationFrameRef.current) {\n                    cancelAnimationFrame(animationFrameRef.current);\n                    animationFrameRef.current = null;\n                }\n                if (sourceRef.current) {\n                    sourceRef.current.disconnect();\n                    sourceRef.current = null;\n                }\n                if (audioContextRef.current) {\n                    audioContextRef.current.close().catch(() => {});\n                    audioContextRef.current = null;\n                }\n                analyserRef.current = null;\n                smoothedLevelRef.current = 0;\n                setLevel(0);\n                setIsMonitoring(false);\n            }\n            return;\n        }\n\n        // If stream exists and autoStart is enabled, start monitoring\n        if (autoStart && !isMonitoring) {\n            start();\n        }\n\n        // Cleanup on stream change\n        return () => {\n            if (intervalRef.current) {\n                clearInterval(intervalRef.current);\n                intervalRef.current = null;\n            }\n            if (animationFrameRef.current) {\n                cancelAnimationFrame(animationFrameRef.current);\n                animationFrameRef.current = null;\n            }\n            if (sourceRef.current) {\n                sourceRef.current.disconnect();\n                sourceRef.current = null;\n            }\n            if (audioContextRef.current) {\n                audioContextRef.current.close().catch(() => {});\n                audioContextRef.current = null;\n            }\n            analyserRef.current = null;\n            smoothedLevelRef.current = 0;\n            setLevel(0);\n            setIsMonitoring(false);\n        };\n    }, [stream, autoStart]); // eslint-disable-line react-hooks/exhaustive-deps\n\n    // Cleanup on unmount\n    useEffect(() => {\n        return () => {\n            stop();\n        };\n    }, []); // eslint-disable-line react-hooks/exhaustive-deps\n\n    return {\n        level,\n        isSpeaking: level > speakingThreshold,\n        peak,\n        resetPeak,\n        start,\n        stop,\n        isMonitoring,\n        isSupported,\n    };\n}\n\nexport default useAudioLevel;\n"
    }
  ]
}